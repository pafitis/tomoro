{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomoro assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "I outline how I am planning to tackle this assessment. At a high-level, I want to educate myself with what the paper is trying to achieve. After I have a basic understanding of what the authors are doing, I want to familiarise myself with the data. With this, basic modelling can act as our benchmark - which we can then enhance to reach an improved model.\n",
    "\n",
    "Attack plan:\n",
    "1. Read paper\n",
    "2. Investigate what data we are working with\n",
    "3. Think about what modelling is relevant\n",
    "4. Start writing utils\n",
    "    - in parallel look at paper + deep-dive data examples\n",
    "6. Basic benchmarking\n",
    "7. Improvements\n",
    "8. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary paper overview\n",
    "\n",
    "First pass of reading the paper. I present my notes below, for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Authors create a new dataset, `ConvFINQA`\n",
    "- Conversational  question/answer type dataset\n",
    "- Questions require multi-step reasoning, specifically within the context of maths/finance\n",
    "- Specialised domain versus general domain reasoning\n",
    "- Answering questions requires context which is either given as text or tables\n",
    "\n",
    "Multihop reasoning is a difficult task to model\n",
    "- These questions require multiple operations until you reach the answer\n",
    "    - Example: what is the % increase of variable X w.r.t to its value last year?\n",
    "        - you need to know the variable X given at the current and last year; then calculate a % change\n",
    "- Two types of questions\n",
    "    - Simple: single multi-hop question (single isolated question that requires multiple operations to answer)\n",
    "    - Hybrid: multiple multi-hop questions (require cross-question reasoning/ dependencies)\n",
    "- Different math operations\n",
    "    - Add, subtract, divide, multiply\n",
    "\n",
    "How modelling is approached\n",
    "- Neural symbolic approach: \n",
    "    - Combines a retriever to find the relevant facts/context; then a generator that uses question + context to decode the \"reasoning program\"\n",
    "    - Reasoning program: They define it as a collection of operations. `op(arg1, arg2)` \n",
    "        - think of this as a functional expression of the reasoning required to solve the \n",
    "- Generative GPT like:\n",
    "    - Relies on the context given during prompting \"gold supporting facts\"\n",
    "        - Bad context, horrible output\n",
    "    - Instructs the output to be like the reasoning program via examples\n",
    "    - Investigates chain-of-thought\n",
    "\n",
    "Key learnings:\n",
    "- The model struggles with long reasoning chains.\n",
    "- The model excels at number selection questions.\n",
    "- The model suffers from the lack of domain knowledge.\n",
    "- GPT-3 can do simple calculations by itself.\n",
    "- GPT-3 performs better for its familiar program format.\n",
    "- GPT-3 struggles with new complex task paradigms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I describe what I've learned through a preliminary data exploration. \n",
    "\n",
    "\n",
    "*Note: Data exploration can be messy. For this reason, I separate the actual work into a different script. Here, I report what I've learned. For further detail, please look at `scripts/data_exploration.ipynb`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pre_text      [26 | 2009 annual report in fiscal 2008 , reve...\n",
       "post_text     [year ended june 30 , cash provided by operati...\n",
       "filename                                  JKHY/2009/page_28.pdf\n",
       "table_ori     [[, Year ended June 30, 2009], [2008, 2007], [...\n",
       "table         [[2008, year ended june 30 2009 2008, year end...\n",
       "qa            {'question': 'what was the percentage change i...\n",
       "id                               Single_JKHY/2009/page_28.pdf-3\n",
       "annotation    {'amt_table': '<table class='wikitable'><tr><t...\n",
       "qa_0                                                        NaN\n",
       "qa_1                                                        NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_json('../data/train.json')\n",
    "raw_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must knows:\n",
    "1. Questions are stored under `.qa` for simple questions (single multi-hop) or `.qa_0` and `.qa_1` for hybrid questions (cross-question dependencies)\n",
    "2. Each entry has an associated `.table` ; `.pre_text` and `.post_text` attribute that is used to construct our \"context\" - relevant during prompting\n",
    "    - Basically this would look like a concatenation of `pre_text` + `table` + `post_text`\n",
    "3. Each entry has also an `.annotation` attribute which contains the major information for the conversations.\n",
    "    - Different information is captured if the entry is a simple question versus a hybrid question\n",
    "    - Importantly, contains the `.dialogue_break` attribute, which splits the question(-s) into simpler subquestions, used to reach the answer\n",
    "    - Contains `.qa_split` to tell you which info corresponds to which question in the case of hybrid questions\n",
    "        - for example, exe_ans_list = [0,0,0,1,1] can be read as the first three elements of the lists correspond to `.qa_0` and the latter two correspond to `.qa_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short example:\n",
      "\n",
      "    .qa.question:\n",
      "     what was the percentage change in the net cash from operating activities from 2008 to 2009\n",
      "\n",
      "    .annotation.dialogue_break:\n",
      "     ['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "    table:\n",
      "     [['2008', 'year ended june 30 2009 2008', 'year ended june 30 2009 2008', 'year ended june 30 2009'], ['net income', '$ 103102', '$ 104222', '$ 104681'], ['non-cash expenses', '74397', '70420', '56348'], ['change in receivables', '21214', '-2913 ( 2913 )', '-28853 ( 28853 )'], ['change in deferred revenue', '21943', '5100', '24576'], ['change in other assets and liabilities', '-14068 ( 14068 )', '4172', '17495'], ['net cash from operating activities', '$ 206588', '$ 181001', '$ 174247']]\n",
      "\n",
      "    pre_text (excerpt):\n",
      "     26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business ...\n",
      "\n",
      "    post_text (excerpt):\n",
      "     year ended june 30 , cash provided by operations increased $ 25587 to $ 206588 for the fiscal year e...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Short example:\\n')\n",
    "print(f'    .qa.question:\\n     {raw_data.iloc[0].qa.get(\"question\")}\\n')\n",
    "print(f'    .annotation.dialogue_break:\\n     {raw_data.iloc[0].annotation.get(\"dialogue_break\")}\\n')\n",
    "print(f'    table:\\n     {raw_data.iloc[0].table}\\n')\n",
    "print(f'    pre_text (excerpt):\\n     {raw_data.iloc[0].pre_text[0][:100]}...\\n')\n",
    "print(f'    post_text (excerpt):\\n     {raw_data.iloc[0].post_text[0][:100]}...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note:\n",
    "1. I've already found some bad examples - The data isn't perfect!\n",
    "    - Cases where the maths operations don't seem to be correct\n",
    "    - Some column names are repeated; effectively not being able to distinguish what numbers are\n",
    "\n",
    "2. We need to convert the table into something parsable by the LLM\n",
    "    - Sometimes column names are supplied sometimes are not\n",
    "    - I've spent sometime to think about it but there doesn't appear to be a global solution for this\n",
    "    - You either assume that first entries are always column names or don't\n",
    "    - Other option is to check the similarity of the first entries versus the rest of the table (in an attempt to understand if its a column name versus a cell value -- but this seems like a very computationally expensive operation)\n",
    "3. Seems like the `pre_text` and `post_text` can be very noisy / dirty\n",
    "    - Not always needed, but you can't know this apriori\n",
    "    - There's the option of the `.gold_inds` but this feels like cheating as its the perfect retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `utils`, which at a high level include the data preprocessing infra, model infra, and other evaluator functions, can be found in the utils folder.\n",
    "\n",
    "Please look at each respective file for explanation of their contents - this report will not go through in detail, unless relevant/ interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to write the basic functionality to parse all this information coherently, and run a few benchmarks.\n",
    "I'm thinking I'll take the API approach as my computer cannot run much..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
