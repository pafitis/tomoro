{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomoro assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "I outline how I am planning to tackle this assessment. At a high-level, I want to educate myself with what the paper is trying to achieve. After I have a basic understanding of what the authors are doing, I want to familiarise myself with the data. With this, basic modelling can act as our benchmark - which we can then enhance to reach an improved model.\n",
    "\n",
    "Attack plan:\n",
    "1. Read paper\n",
    "2. Investigate what data we are working with\n",
    "3. Think about what modelling is relevant\n",
    "4. Start writing utils\n",
    "    - in parallel look at paper + deep-dive data examples\n",
    "6. Basic benchmarking\n",
    "7. Improvements\n",
    "8. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary paper overview\n",
    "\n",
    "First pass of reading the paper. I present my notes below, for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Authors create a new dataset, `ConvFINQA`\n",
    "- Conversational  question/answer type dataset\n",
    "- Questions require multi-step reasoning, specifically within the context of maths/finance\n",
    "- Specialised domain versus general domain reasoning\n",
    "- Answering questions requires context which is either given as text or tables\n",
    "\n",
    "Multihop reasoning is a difficult task to model\n",
    "- These questions require multiple operations until you reach the answer\n",
    "    - Example: what is the % increase of variable X w.r.t to its value last year?\n",
    "        - you need to know the variable X given at the current and last year; then calculate a % change\n",
    "- Two types of questions\n",
    "    - Simple: single multi-hop question (single isolated question that requires multiple operations to answer)\n",
    "    - Hybrid: multiple multi-hop questions (require cross-question reasoning/ dependencies)\n",
    "- Different math operations\n",
    "    - Add, subtract, divide, multiply, power\n",
    "\n",
    "How modelling is approached\n",
    "- Neural symbolic approach: \n",
    "    - Combines a retriever to find the relevant facts/context; then a generator that uses question + context to decode the \"reasoning program\"\n",
    "    - Reasoning program: They define it as a collection of operations. `op(arg1, arg2)` \n",
    "        - think of this as a functional expression of the reasoning required to solve the \n",
    "- Generative GPT like:\n",
    "    - Relies on the context given during prompting \"gold supporting facts\"\n",
    "        - Bad context, horrible output\n",
    "    - Instructs the output to be like the reasoning program via examples\n",
    "    - Investigates chain-of-thought\n",
    "\n",
    "Key learnings:\n",
    "- The model struggles with long reasoning chains.\n",
    "- The model excels at number selection questions.\n",
    "- The model suffers from the lack of domain knowledge.\n",
    "- GPT-3 can do simple calculations by itself.\n",
    "- GPT-3 performs better for its familiar program format.\n",
    "- GPT-3 struggles with new complex task paradigms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I describe what I've learned through a preliminary data exploration. \n",
    "\n",
    "\n",
    "*Note: Data exploration can be messy. For this reason, I separate the actual work into a different script. Here, I report what I've learned. For further detail, please look at `scripts/data_exploration.ipynb`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pre_text      [26 | 2009 annual report in fiscal 2008 , reve...\n",
       "post_text     [year ended june 30 , cash provided by operati...\n",
       "filename                                  JKHY/2009/page_28.pdf\n",
       "table_ori     [[, Year ended June 30, 2009], [2008, 2007], [...\n",
       "table         [[2008, year ended june 30 2009 2008, year end...\n",
       "qa            {'question': 'what was the percentage change i...\n",
       "id                               Single_JKHY/2009/page_28.pdf-3\n",
       "annotation    {'amt_table': '<table class='wikitable'><tr><t...\n",
       "qa_0                                                        NaN\n",
       "qa_1                                                        NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_json('../data/train.json')\n",
    "raw_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must knows:\n",
    "1. Questions are stored under `.qa` for simple questions (single multi-hop) or `.qa_0` and `.qa_1` for hybrid questions (cross-question dependencies)\n",
    "2. Each entry has an associated `.table` ; `.pre_text` and `.post_text` attribute that is used to construct our \"context\" - relevant during prompting\n",
    "    - Basically this would look like a concatenation of `pre_text` + `table` + `post_text`\n",
    "3. Each entry has also an `.annotation` attribute which contains the major information for the conversations.\n",
    "    - Different information is captured if the entry is a simple question versus a hybrid question\n",
    "    - Importantly, contains the `.dialogue_break` attribute, which splits the question(-s) into simpler subquestions, used to reach the answer\n",
    "    - Contains `.qa_split` to tell you which info corresponds to which question in the case of hybrid questions\n",
    "        - for example, exe_ans_list = [0,0,0,1,1] can be read as the first three elements of the lists correspond to `.qa_0` and the latter two correspond to `.qa_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short example:\n",
      "\n",
      "    .qa.question:\n",
      "     what was the percentage change in the net cash from operating activities from 2008 to 2009\n",
      "\n",
      "    .annotation.dialogue_break:\n",
      "     ['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "    table:\n",
      "     [['2008', 'year ended june 30 2009 2008', 'year ended june 30 2009 2008', 'year ended june 30 2009'], ['net income', '$ 103102', '$ 104222', '$ 104681'], ['non-cash expenses', '74397', '70420', '56348'], ['change in receivables', '21214', '-2913 ( 2913 )', '-28853 ( 28853 )'], ['change in deferred revenue', '21943', '5100', '24576'], ['change in other assets and liabilities', '-14068 ( 14068 )', '4172', '17495'], ['net cash from operating activities', '$ 206588', '$ 181001', '$ 174247']]\n",
      "\n",
      "    pre_text (excerpt):\n",
      "     26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business ...\n",
      "\n",
      "    post_text (excerpt):\n",
      "     year ended june 30 , cash provided by operations increased $ 25587 to $ 206588 for the fiscal year e...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Short example:\\n')\n",
    "print(f'    .qa.question:\\n     {raw_data.iloc[0].qa.get(\"question\")}\\n')\n",
    "print(f'    .annotation.dialogue_break:\\n     {raw_data.iloc[0].annotation.get(\"dialogue_break\")}\\n')\n",
    "print(f'    table:\\n     {raw_data.iloc[0].table}\\n')\n",
    "print(f'    pre_text (excerpt):\\n     {raw_data.iloc[0].pre_text[0][:100]}...\\n')\n",
    "print(f'    post_text (excerpt):\\n     {raw_data.iloc[0].post_text[0][:100]}...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note:\n",
    "1. I've already found some bad examples - The data isn't perfect!\n",
    "    - Cases where the maths operations don't seem to be correct\n",
    "    - Some column names are repeated; effectively not being able to distinguish what numbers are\n",
    "    - **This means that the accuracy metrics at the end should be taken with a grain of salt!**\n",
    "\n",
    "2. We need to convert the table into something parsable by the LLM\n",
    "    - Sometimes column names are supplied sometimes are not\n",
    "    - I've spent sometime to think about it but there doesn't appear to be a global solution for this\n",
    "    - You either assume that first entries are always column names or don't\n",
    "    - Other option is to check the similarity of the first entries versus the rest of the table (in an attempt to understand if its a column name versus a cell value -- but this seems like a very computationally expensive operation)\n",
    "3. Seems like the `pre_text` and `post_text` can be very noisy / dirty\n",
    "    - Not always needed, but you can't know this apriori\n",
    "    - There's the option of the `.gold_inds` but this feels like cheating as its the perfect retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant utilities can be found in the `utils` folder, which at a high level include the data preprocessing infra, model infra, and other evaluator functions.\n",
    "\n",
    "Please look at each respective file for explanation of their contents - this report will not go through in detail, unless relevant/ interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to write the basic functionality to parse all this information coherently, and run a few benchmarks.\n",
    "I'm thinking I'll take the API approach as my computer cannot run much...\n",
    "\n",
    "I am relying to achieve most of the performance improvement through prompt engineering. I present here the thought process of how each prompt was structured and the underlying strategy.\n",
    "\n",
    "I am using Deepseek's models as they are the cheapest at the moment of writing. I have allocated 5-10$ for this assessment and generate an API key to use their models. I do this by using OpenAI's SDK as suggested by the original Deepseek docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering\n",
    "\n",
    "There are some basics that we want to force our LLM to do. \n",
    "1. Use only the context it was provided and not rely on any external information\n",
    "2. We want to enforce some output format to be able to easily extract the answer\n",
    "\n",
    "#### Prompt 01\n",
    "\n",
    "The first prompt, naively asks the LLM to give the numerical output. This was my first attempt to just get a basic understanding if the `utils` work in general and what the LLM can or cannot do.\n",
    "\n",
    "I used a system prompt, asking the LLM to act as a financial analysis expert - this is because the questions revolve around finance, and if not directly relevant, still use mathematics. All of which are mostly found in financial data. This is our attempt to trigger the relevant weights that correspond to that type of \"thinking\".\n",
    "\n",
    "*It is important to note that asking the LLM to do maths directly isn't smart. This is apparent immediately, and it will be resolved but we are still in exploration mode. Even if its reasoning it correct, the actual evaluation of operations is unlikely to be correct. Purely out of how LLMs work underneath + the data it has seen.*\n",
    "\n",
    "```\n",
    "SYSTEM_PROMPT_V1 = r\"\"\"\n",
    "    Act as a financial analysis specialist. Your responses must:\n",
    "    1. Strictly use only the contextual information provided by the user\n",
    "    2. Deliver the final answer in this exact format:\n",
    "        - Unitless numerical value\n",
    "        - Enclosed in \\boxed{} LaTeX formatting\n",
    "\n",
    "    Never reference external knowledge or assumptions. \n",
    "    Convert all scaled values to absolute numbers during calculations,\n",
    "    but omit units in the final answer.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Prompt 02\n",
    "\n",
    "Very similar to the first, still do not ask it to give maths operators - still evaluates the answer directly. The difference here is I want to extract its reasoning. This was an attempt to see what the output is like, and consider if I can use it to extract it for the user. The user likely wants to know where in the document that information is present.\n",
    "\n",
    "This evolves a bit more, in the later stages so won't be described in detail here. Look for how context is extracted in the `Structured output` section.\n",
    "\n",
    "This allowed me to explore a bit how the thought process is presented/ structured.\n",
    "\n",
    "```\n",
    "SYSTEM_PROMPT_V2 = r\"\"\"\n",
    "    Act as a financial analysis specialist. Your responses must:\n",
    "    1. Strictly use only the contextual information provided by the user\n",
    "    2. Explicitly show your logical reasoning process through sequential step-by-step explanations\n",
    "    3. Deliver the final answer in this exact format:\n",
    "        - Unitless numerical value\n",
    "        - Enclosed in \\boxed{} LaTeX formatting\n",
    "\n",
    "    Never reference external knowledge or assumptions. \n",
    "    Convert all scaled values to absolute numbers during calculations, \n",
    "    but omit units in the final answer.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Prompt 03\n",
    "\n",
    "We now ask the LLM to use a selection of pre-defined maths operations, and NEVER evaluate the answer. This means we expect the LLM to give an output of looking like `add(a, b)` or similar.\n",
    "\n",
    "For this to work, we slightly modify the prompt but keep all previous asks (use context only + ignore units + output format).\n",
    "\n",
    "I've researched a bit on how to structure the language and landed on this approach. Language here can be varied and is up to the developer's approach. It does impact the LLM but it is difficult to know exactly what words/language will lead to the best output without iteratively testing each one.\n",
    "\n",
    "Importantly:\n",
    "1. I define the operators\n",
    "2. I explicitly mention that I am expecting nested operators rather than new lines/ evaluated intermediate steps\n",
    "3. I define that each operator ONLY takes two arguments \n",
    "4. Give a short example\n",
    "\n",
    "```\n",
    "SYSTEM_PROMPT_V3 = r\"\"\"\n",
    "    Act as a financial computation engine. Required behavior:\n",
    "    1. Input Processing:\n",
    "    - Use ONLY context provided in the query\n",
    "    - Never incorporate external data or assumptions\n",
    "    2. Calculation Methodology:\n",
    "    - Perform and display calculations by using ONLY these Python-style operators:\n",
    "        - add(a, b) → a + b\n",
    "        - subtract(a, b) → a - b\n",
    "        - multiply(a, b) → a * b\n",
    "        - divide(a, b) → a / b\n",
    "        - power(a, b) → a^b\n",
    "    - Each operator must have EXACTLY two arguments\n",
    "    3. Output Requirements:\n",
    "    - Final answer must be:\n",
    "        - A nested combination of allowed operators\n",
    "        - In unevaluated functional form\n",
    "        - Expressed as \\boxed{operator(...)} LaTeX\n",
    "    - Include intermediate unit normalization calculations\n",
    "\n",
    "    Example: For \"Revenue per share - Cost per share = (5,000,000 revenue / 2,000,000 shares) - $5\"\n",
    "    Acceptable: \\boxed{subtract(divide(5000000, 2000000), 5)}\n",
    "    Unacceptable: \\boxed{2.5 - 5} or \\boxed{-2.5}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "With this, I write a function called `evaluate_maths` that will be taking the string output of the LLM, and using python's `eval` method to evaluate these operators. I also write a dictionary of these maths operators (exactly as seen in the prompt) for python to use.\n",
    "\n",
    "\n",
    "#### Prompt 04: Structured output\n",
    "\n",
    "**Note: Structured output is usually coded by providing a json schema + pydantic objects. However, based on Deepseek's documentation, they expect this to be given during prompting.**\n",
    "\n",
    "**See here: https://api-docs.deepseek.com/guides/json_mode**\n",
    "\n",
    "**For this reason, I provide a prompt that handles structured outputs and will consider if a pydantic implementation is required (if say I use other model providers)**\n",
    "\n",
    "This was necessitated after a lot of investigation of how the outputs are produced. \n",
    "\n",
    "Structured outputs is the mechanism of forcing your LLM to produce its output in a very defined way. The LLM is given a `json` schema and it's output will abide to it. With this, we can force specific attributes, for example quoting where the relevant text is, the thought process, and the final `program` to be evaluated. Without this approach, the LLM sometimes hallucinates. This is also a cause of \"randomness\" in a way (even if you set temperature to 0.0)\n",
    "\n",
    "```\n",
    "SYSTEM_PROMPT_V4 = r\"\"\"\n",
    "    Act as a financial computation engine that outputs valid JSON. Required behavior:\n",
    "    1. Input Processing:\n",
    "    - Use ONLY context provided in the query\n",
    "    - Never incorporate external data or assumptions\n",
    "    2. Calculation Methodology:\n",
    "    - Perform and display calculations by using ONLY these Python-style operators:\n",
    "        - add(a, b) → a + b\n",
    "        - subtract(a, b) → a - b\n",
    "        - multiply(a, b) → a * b\n",
    "        - divide(a, b) → a / b\n",
    "        - power(a, b) → a^b\n",
    "    - Each operator must have EXACTLY two arguments\n",
    "    3. JSON Output Requirements:\n",
    "    - Structure response as valid JSON with this schema:\n",
    "        {\n",
    "            \"user_question\": \"string\",\n",
    "            \"user_context\": \"string\",\n",
    "            \"reasoning\": [\"step1\", \"step2\", ..., \"stepN\"],\n",
    "            \"final_answer\": \"boxed_expression\"\n",
    "        }\n",
    "    - Maintain atomic values in JSON (no complex objects)\n",
    "    - Escape special characters properly\n",
    "    - final_answer must use: \\boxed{operator(...)} format\n",
    "    \n",
    "    4. Compliance:\n",
    "    - Strictly follow JSON syntax\n",
    "    - No markdown formatting\n",
    "    - No additional explanations outside JSON structure\n",
    "\n",
    "    Example valid response:\n",
    "    {\n",
    "        \"user_question\": \"Calculate profit per share given 5M revenue and 2M shares with $5 fixed cost\",\n",
    "        \"user_context\": \"[Row 1] Revenue: 5,000,000\\n[Row 2] Shares: 2,000,000\\n[Row 3] Fixed cost per share: 5\",\n",
    "        \"reasoning\": [\n",
    "            \"1. Revenue per share - Cost per share\", \n",
    "            \"2. Convert 5M revenue to 5,000,000\",\n",
    "            \"3. Divide revenue by shares: 5,000,000/2,000,000\",\n",
    "            \"4. Subtract fixed cost per share from revenue per share\",\n",
    "            \"4. Use subtract() for subtraction and divide() for division\"\n",
    "        ],\n",
    "        \"final_answer\": \"\\boxed{subtract(divide(5000000, 2000000), 5)}\"\n",
    "    }\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
